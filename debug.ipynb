{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5737e64c-78d3-4faf-b427-ee81d988b653",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb84ccc-7407-48f2-8fea-b1aad6f2fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                                                                                        | 0/300 [00:00<?, ?ep/s]\n",
      "Train ep1:   0%|                                                                                                                                                   | 0/2899 [00:00<?, ?it/s]\u001b[A\n",
      "Train ep1:   0%|                                                                                                                                         | 1/2899 [00:01<1:17:03,  1.60s/it]\u001b[A\n",
      "Train ep1:   0%|                                                                                                                                           | 2/2899 [00:02<54:34,  1.13s/it]\u001b[A\n",
      "Train ep1:   0%|▏                                                                                                                                          | 3/2899 [00:03<46:30,  1.04it/s]\u001b[A\n",
      "Train ep1:   0%|▏                                                                                                                                          | 4/2899 [00:03<42:46,  1.13it/s]\u001b[A\n",
      "Train ep1:   0%|▏                                                                                                                                          | 5/2899 [00:04<40:39,  1.19it/s]\u001b[A\n",
      "Train ep1:   0%|▎                                                                                                                                          | 6/2899 [00:05<39:26,  1.22it/s]\u001b[A\n",
      "Train ep1:   0%|▎                                                                                                                                          | 7/2899 [00:06<38:42,  1.25it/s]\u001b[A\n",
      "Train ep1:   0%|▍                                                                                                                                          | 8/2899 [00:07<38:10,  1.26it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shlex\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "from model import HorizonNet, ENCODER_RESNET, ENCODER_DENSENET\n",
    "from dataset import PanoCorBonDataset, ZillowIndoorDataset, ZillowIndoorPairDataset\n",
    "from misc.utils import group_weight, adjust_learning_rate, save_model, load_trained_model\n",
    "from inference import inference\n",
    "from eval_general import test_general\n",
    "from transformations_torch import *\n",
    "from grid import warp_index, compute_local, compute_global\n",
    "\n",
    "def check_parameters(net):\n",
    "    for p in net.parameters():\n",
    "        if torch.all(p.isnan()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val = val\n",
    "        self.sum += val * num\n",
    "        self.count += num\n",
    "        if self.count > 0:\n",
    "            self.avg = self.sum / self.count\n",
    "        else: \n",
    "            self.avg = 0\n",
    "\n",
    "def nan_to_num(x):\n",
    "    nan_map = x.isnan()\n",
    "    res = nan_map.float() #(N, 2, 1024)\n",
    "    res[:, 0, :] *= -0.478\n",
    "    res[:, 1, :] *= 0.425\n",
    "\n",
    "    x[nan_map] = 0.\n",
    "\n",
    "    return x + res    \n",
    "\n",
    "def clip_bon(bon, H):\n",
    "    # bon.shape = (N, 2, 1024)\n",
    "    ceil = torch.clamp(bon[:, 0, :], 0.5, H/2 - 45)\n",
    "    floor = torch.clamp(bon[:, 1, :], H/2 + 45, H - 0.5)\n",
    "\n",
    "    return torch.stack((ceil, floor), dim=1)\n",
    "\n",
    "\n",
    "def feed_forward(net, criterion, params, single=False, epoch=0):\n",
    "\n",
    "    src_img, src_rotation_matrix, src_scale, src_translation, target_img, target_rotation_matrix, target_scale, target_translation, stretched_src_img, stretched_target_img, stretch_k, ceiling_height = params\n",
    "    if single:\n",
    "        src_img = src_img[None]\n",
    "        src_rotation_matrix = src_rotation_matrix[None]\n",
    "        src_scale = src_scale[None]\n",
    "        src_translation = src_translation[None]\n",
    "        target_img = target_img[None]\n",
    "        target_rotation_matrix = target_rotation_matrix[None]\n",
    "        target_scale = target_scale[None]\n",
    "        target_translation = target_translation[None]\n",
    "        stretched_src_img = stretched_src_img[None]\n",
    "        stretched_target_img = stretched_target_img[None]\n",
    "        stretch_k = stretch_k[None]\n",
    "        ceiling_height = ceiling_height[None]\n",
    "    \n",
    "    src_img = src_img.to(device)\n",
    "    src_rotation_matrix = src_rotation_matrix.to(device)\n",
    "    src_scale = src_scale.to(device)\n",
    "    src_translation = src_translation.to(device)\n",
    "    target_img = target_img.to(device)\n",
    "    target_rotation_matrix = target_rotation_matrix.to(device)\n",
    "    target_scale = target_scale.to(device)\n",
    "    target_translation = target_translation.to(device)\n",
    "    # stretched_src_img = stretched_src_img.to(device)\n",
    "    stretched_target_img = stretched_target_img.to(device)\n",
    "    stretch_k = stretch_k.to(device)\n",
    "    ceiling_height = ceiling_height.to(device)\n",
    "\n",
    "    N, _, H, W = src_img.shape\n",
    "\n",
    "    # assert not torch.any(torch.isnan(src_img))\n",
    "    # assert not torch.any(torch.isnan(target_img))\n",
    "\n",
    "    # y_bon_ori = nan_to_num(net(target_img))\n",
    "    y_bon_ori = net(target_img)\n",
    "    assert not torch.all(torch.isnan(y_bon_ori))\n",
    "    y_bon_ori = nan_to_num(y_bon_ori)\n",
    "\n",
    "    # y_bon = (y_bon_ori / np.pi + 0.5) * H - 0.5\n",
    "    y_bon = (y_bon_ori / 2 + 0.5) * H - 0.5\n",
    "    # y_bon = y_bon_ori * H - 0.5\n",
    "    y_bon = clip_bon(y_bon, H)\n",
    "    # y_bon = nan_to_num(y_bon)\n",
    "\n",
    "    # y_bon_src = nan_to_num(net(src_img))\n",
    "    # y_bon_src = (y_bon_src / 2 + 0.5) * H - 0.5\n",
    "    # y_bon_src = clip_bon(y_bon_src, H)\n",
    "\n",
    "    # assert not torch.any(torch.isnan(y_bon))\n",
    "\n",
    "    losses = {} \n",
    "\n",
    "    src_transformer = Transformation2D(rotation_matrix=src_rotation_matrix, scale=src_scale[:, :, None], translation=src_translation)\n",
    "    target_transformer = Transformation2D(rotation_matrix=target_rotation_matrix, scale=target_scale[:, :, None], translation=target_translation)\n",
    "    \n",
    "    ceiling_z = ceiling_height - 1.\n",
    "\n",
    "    grid = warp_index(src_transformer, target_transformer, y_bon, H, W, ceiling_z) \n",
    "    assert not torch.any(torch.isnan(grid))    \n",
    "\n",
    "    warp_img = F.grid_sample(src_img, grid)\n",
    "    losses['ph'] = criterion(warp_img, target_img)\n",
    "    assert not torch.any(torch.isnan(warp_img))\n",
    "\n",
    "\n",
    "\n",
    "    #warp_img = torch.cat([target_img[:, :, :256, :], warp_img, target_img[:, :, 480:, :]], dim=2)\n",
    "    pseudo_y_bon_ori = nan_to_num(net(warp_img))\n",
    "    # pseudo_y_bon_ori = net(warp_img)\n",
    "    # pseudo_y_bon = (pseudo_y_bon_ori / np.pi + 0.5) * H - 0.5\n",
    "    pseudo_y_bon = (pseudo_y_bon_ori / 2 + 0.5) * H - 0.5\n",
    "    # pseudo_y_bon = pseudo_y_bon_ori * H - 0.5\n",
    "    pseudo_y_bon = clip_bon(pseudo_y_bon, H)\n",
    "    # pseudo_y_bon = nan_to_num(pseudo_y_bon)\n",
    "\n",
    "\n",
    "    # assert not torch.any(torch.isnan(pseudo_y_bon))\n",
    "    # assert not torch.any(torch.isnan(pseudo_y_bon_ori))\n",
    "    # assert not torch.any(torch.isnan(y_bon_ori))\n",
    "\n",
    "    target_global_2d = compute_global(y_bon, target_transformer, H, W, ceiling_z)\n",
    "    \n",
    "\n",
    "    target_local_2d = target_transformer.apply_inverse(target_global_2d.clone()) \n",
    "    target_global_stretch = target_transformer.to_global(target_local_2d.clone() * stretch_k[:, None, :])\n",
    "    target_local_stretch = target_transformer.apply_inverse(target_global_stretch) \n",
    "    z = torch.cat([ceiling_z[:, None, :].expand(N, W, 1), torch.zeros(N, W, 1).to(device) - 1.], dim=1)\n",
    "    target_local_3d = torch.cat([target_local_stretch, z], dim=-1)\n",
    "    stretched_y_bon = TransformationSpherical.cartesian_to_pixel(target_local_3d.reshape(-1, 3), 1024).reshape(-1, 2, 1024, 2)[:, :, :, 1]\n",
    "    stretched_y_bon = clip_bon(stretched_y_bon, H)\n",
    "    \n",
    "    stretched_y_bon_ = net(stretched_target_img)\n",
    "    stretched_y_bon_ = (stretched_y_bon_ / 2 + 0.5) * H - 0.5  \n",
    "    stretched_y_bon_ = clip_bon(stretched_y_bon_, H)\n",
    "\n",
    "    target_global_2d -= target_translation\n",
    "\n",
    "    ceil = target_global_2d[:, :1024, :]\n",
    "    floor = target_global_2d[:, 1024:, :]\n",
    "    losses['ceil_floor'] = criterion(ceil, floor)\n",
    "\n",
    "    \n",
    "    kernel_size = 15\n",
    "    unfold = nn.Unfold(kernel_size=(1, kernel_size))\n",
    "    windows = unfold(target_global_2d.reshape(N, 2, W, 2).permute(0, 1, 3, 2)).reshape(N, 2, 2, -1, kernel_size) #(N, C, XY, L, K)\n",
    "    windows_mean = windows.median(dim=-1, keepdim=True).values\n",
    "    windows_slope = windows / windows.norm(dim=2, keepdim=True)\n",
    "    \n",
    "    \n",
    "    losses['bon'] = criterion(pseudo_y_bon_ori, y_bon_ori)\n",
    "    losses['stretch'] = criterion(stretched_y_bon, stretched_y_bon_)\n",
    "\n",
    "    # losses['ud_con'] = chamfer_distance(target_global_2d[:, :1024, :], target_global_2d[:, 1024:, :])[0]\n",
    "    losses['parallel'] = torch.abs((windows - windows_mean) * torch.flip(windows_slope, [2])).mean(dim=4).min(dim=2).values.mean()\n",
    "    # losses['parallel'] = windows.var(dim=4).min(dim=2).values.mean()\n",
    "    # losses['chamfer'] = compute_chamfer(chamfer_distance, y_bon_src,    src_transformer,    y_bon,          target_transformer, H, W)\n",
    "    # losses['chamfer'] = compute_chamfer(chamfer_distance, pseudo_y_bon, target_transformer, y_bon,          target_transformer, H, W) + \\\n",
    "    #                     compute_chamfer(chamfer_distance, y_bon_src,    src_transformer,    y_bon,          target_transformer, H, W) + \\\n",
    "    #                     compute_chamfer(chamfer_distance, y_bon_src,    src_transformer,    pseudo_y_bon,   target_transformer, H, W)\n",
    "    # losses['total'] = losses['ph']\n",
    "    losses['total'] = losses['ph'] + losses['bon'] * 0.1 + losses['parallel'] * 0.1 + losses['ceil_floor'] * 0.15 + losses['stretch'] * 0.05\n",
    "\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--id', required=True,\n",
    "                        help='experiment id to name checkpoints and logs')\n",
    "    parser.add_argument('--ckpt', default='./ckpt',\n",
    "                        help='folder to output checkpoints')\n",
    "    parser.add_argument('--logs', default='./logs',\n",
    "                        help='folder to logging')\n",
    "    parser.add_argument('--pth', default=None,\n",
    "                        help='path to load saved checkpoint.'\n",
    "                             '(finetuning)')\n",
    "    # Model related\n",
    "    parser.add_argument('--backbone', default='resnet50',\n",
    "                        choices=ENCODER_RESNET + ENCODER_DENSENET,\n",
    "                        help='backbone of the network')\n",
    "    parser.add_argument('--no_rnn', action='store_true',\n",
    "                        help='whether to remove rnn or not')\n",
    "    # Dataset related arguments\n",
    "    parser.add_argument('--train_root_dir', default='data/layoutnet_dataset/train',\n",
    "                        help='root directory to training dataset. '\n",
    "                             'should contains img, label_cor subdirectories')\n",
    "    parser.add_argument('--valid_root_dir', default='data/layoutnet_dataset/valid',\n",
    "                        help='root directory to validation dataset. '\n",
    "                             'should contains img, label_cor subdirectories')\n",
    "    parser.add_argument('--no_flip', action='store_true',\n",
    "                        help='disable left-right flip augmentation')\n",
    "    parser.add_argument('--no_rotate', action='store_true',\n",
    "                        help='disable horizontal rotate augmentation')\n",
    "    parser.add_argument('--no_gamma', action='store_true',\n",
    "                        help='disable gamma augmentation')\n",
    "    parser.add_argument('--no_pano_stretch', action='store_true',\n",
    "                        help='disable pano stretch')\n",
    "    parser.add_argument('--num_workers', default=6, type=int,\n",
    "                        help='numbers of workers for dataloaders')\n",
    "    # optimization related arguments\n",
    "    parser.add_argument('--freeze_earlier_blocks', default=-1, type=int)\n",
    "    parser.add_argument('--batch_size_train', default=4, type=int,\n",
    "                        help='training mini-batch size')\n",
    "    parser.add_argument('--batch_size_valid', default=2, type=int,\n",
    "                        help='validation mini-batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int,\n",
    "                        help='epochs to train')\n",
    "    parser.add_argument('--optim', default='Adam',\n",
    "                        help='optimizer to use. only support SGD and Adam')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_pow', default=0.9, type=float,\n",
    "                        help='power in poly to drop LR')\n",
    "    parser.add_argument('--warmup_lr', default=1e-6, type=float,\n",
    "                        help='starting learning rate for warm up')\n",
    "    parser.add_argument('--warmup_epochs', default=0, type=int,\n",
    "                        help='numbers of warmup epochs')\n",
    "    parser.add_argument('--beta1', default=0.9, type=float,\n",
    "                        help='momentum for sgd, beta1 for adam')\n",
    "    parser.add_argument('--weight_decay', default=0, type=float,\n",
    "                        help='factor for L2 regularization')\n",
    "    parser.add_argument('--bn_momentum', type=float)\n",
    "    # Misc arguments\n",
    "    parser.add_argument('--no_cuda', action='store_true',\n",
    "                        help='disable cuda')\n",
    "    parser.add_argument('--seed', default=594277, type=int,\n",
    "                        help='manual seed')\n",
    "    parser.add_argument('--disp_iter', type=int, default=1,\n",
    "                        help='iterations frequency to display')\n",
    "    parser.add_argument('--save_every', type=int, default=25,\n",
    "                        help='epochs frequency to save state_dict')\n",
    "    options = '--id stretch --train_root_dir /mnt/home_6T/sunset/zind --valid_root_dir /mnt/home_6T/sunset/zind --no_flip --no_rotate --no_gamma --batch_size_train 2'\n",
    "    args = parser.parse_args(shlex.split(options))\n",
    "    device = torch.device('cpu' if args.no_cuda else 'cuda')\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    os.makedirs(os.path.join(args.ckpt, args.id), exist_ok=True)\n",
    "\n",
    "    torch.manual_seed(2021)\n",
    "\n",
    "\n",
    "    # Create dataloader\n",
    "    # dataset_train = PanoCorBonDataset(\n",
    "    dataset_train = ZillowIndoorPairDataset(\n",
    "        root_dir=args.train_root_dir,\n",
    "        subject='train',\n",
    "        flip=not args.no_flip, rotate=not args.no_rotate, gamma=not args.no_gamma,\n",
    "        stretch=not args.no_pano_stretch)\n",
    "    loader_train = DataLoader(dataset_train, args.batch_size_train,\n",
    "                              shuffle=False, drop_last=True,\n",
    "                              num_workers=args.num_workers,\n",
    "                              pin_memory=not args.no_cuda,\n",
    "                              worker_init_fn=lambda x: np.random.seed())\n",
    "    if args.valid_root_dir:\n",
    "        # dataset_valid = PanoCorBonDataset(\n",
    "        dataset_valid = ZillowIndoorPairDataset(\n",
    "            root_dir=args.valid_root_dir, subject='val', return_cor=True,\n",
    "            flip=False, rotate=False, gamma=False,\n",
    "            stretch=False)\n",
    "\n",
    "    # Create model\n",
    "    if args.pth is not None:\n",
    "        print('Finetune model is given.')\n",
    "        print('Ignore --backbone and --no_rnn')\n",
    "        net = load_trained_model(HorizonNet, args.pth).to(device)\n",
    "    else:\n",
    "        net = HorizonNet(args.backbone, not args.no_rnn).to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    assert -1 <= args.freeze_earlier_blocks and args.freeze_earlier_blocks <= 4\n",
    "    if args.freeze_earlier_blocks != -1:\n",
    "        b0, b1, b2, b3, b4 = net.feature_extractor.list_blocks()\n",
    "        blocks = [b0, b1, b2, b3, b4]\n",
    "        for i in range(args.freeze_earlier_blocks + 1):\n",
    "            print('Freeze block%d' % i)\n",
    "            for m in blocks[i]:\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    if args.bn_momentum:\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                m.momentum = args.bn_momentum\n",
    "\n",
    "    # Create optimizer\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, net.parameters()),\n",
    "            lr=args.lr, momentum=args.beta1, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, net.parameters()),\n",
    "            lr=args.lr, betas=(args.beta1, 0.999), weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=7, min_lr=1e-6)\n",
    "    # Create tensorboard for monitoring training\n",
    "    tb_path = os.path.join(args.logs, args.id)\n",
    "    os.makedirs(tb_path, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(log_dir=tb_path)\n",
    "\n",
    "    # Init variable\n",
    "    args.warmup_iters = args.warmup_epochs * len(loader_train)\n",
    "    args.max_iters = args.epochs * len(loader_train)\n",
    "    args.running_lr = args.warmup_lr if args.warmup_epochs > 0 else args.lr\n",
    "    args.cur_iter = 0\n",
    "    args.best_valid_score = 10000\n",
    "\n",
    "    # Start training\n",
    "    for ith_epoch in trange(1, args.epochs + 1, desc='Epoch', unit='ep'):\n",
    "\n",
    "        # Train phase\n",
    "        net.train()\n",
    "        if args.freeze_earlier_blocks != -1:\n",
    "            b0, b1, b2, b3, b4 = net.feature_extractor.list_blocks()\n",
    "            blocks = [b0, b1, b2, b3, b4]\n",
    "            for i in range(args.freeze_earlier_blocks + 1):\n",
    "                for m in blocks[i]:\n",
    "                    m.eval()\n",
    "        iterator_train = iter(loader_train)\n",
    "        for _ in trange(len(loader_train),\n",
    "                        desc='Train ep%s' % ith_epoch, position=1):\n",
    "            # Set learning ratetmux\n",
    "            adjust_learning_rate(optimizer, args)\n",
    "\n",
    "            args.cur_iter += 1\n",
    "            # print('*'*30, args.cur_iter, '*'*30)\n",
    "            params = next(iterator_train)\n",
    "\n",
    "            losses = feed_forward(net, criterion, params, epoch=ith_epoch)\n",
    "            for k, v in losses.items():\n",
    "                k = 'train/%s' % k\n",
    "                tb_writer.add_scalar(k, v.item(), args.cur_iter)\n",
    "            tb_writer.add_scalar('train/lr', args.running_lr, args.cur_iter)\n",
    "            loss = losses['total']\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if check_parameters(net):\n",
    "                break\n",
    "\n",
    "            # for p in net.parameters():\n",
    "            #     if p.grad is not None:\n",
    "            #         print(p.grad)\n",
    "\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), 1.0, norm_type='inf')\n",
    "            optimizer.step()\n",
    "            prev_params = params\n",
    "\n",
    "        # Valid phase\n",
    "#         net.eval()\n",
    "#         if args.valid_root_dir:\n",
    "#             valid_loss = {}\n",
    "#             meters = {n_corner: AverageMeter() for n_corner in ['4', '6', '8', '10+', 'odd']}\n",
    "#             excepts = 0\n",
    "#             loss_meter = AverageMeter()\n",
    "#             for jth in trange(len(dataset_valid),\n",
    "#                             desc='Valid ep%d' % ith_epoch, position=2):\n",
    "#                 if jth > 1000:\n",
    "#                     break\n",
    "#                 # x, y_bon, y_cor, gt_cor_id = dataset_valid[jth]\n",
    "#                 # x, y_bon, y_cor = x[None], y_bon[None], y_cor[None]\n",
    "#                 params = dataset_valid[jth]\n",
    "#                 with torch.no_grad():\n",
    "#                     losses = feed_forward(net, criterion, params, single=True)\n",
    "#                     loss_meter.update(losses['total'].item(), 1)\n",
    "#                     # True eval result instead of training objective\n",
    "#                     # true_eval = dict([\n",
    "#                     #     (n_corner, {'2DIoU': [], '3DIoU': [], 'rmse': [], 'delta_1': []})\n",
    "#                     #     for n_corner in ['4', '6', '8', '10+', 'odd', 'overall']\n",
    "#                     # ])\n",
    "\n",
    "#                     # try:\n",
    "#                     #     dt_cor_id = inference(net, x, device, force_cuboid=False, force_raw=True)[0]\n",
    "#                     #     dt_cor_id[:, 0] *= 1024\n",
    "#                     #     dt_cor_id[:, 1] *= 512\n",
    "#                     # except:\n",
    "#                     #     excepts += 1\n",
    "#                     #     dt_cor_id = np.array([\n",
    "#                     #         [k//2 * 1024, 256 - ((k%2)*2 - 1) * 120]\n",
    "#                     #         for k in range(8)\n",
    "#                     #     ])\n",
    "#                     # # dt_cor_id = inference(net, x, device, force_cuboid=False, force_raw=True)[0]\n",
    "#                     # # dt_cor_id[:, 0] *= 1024\n",
    "#                     # # dt_cor_id[:, 1] *= 512\n",
    "\n",
    "#                     # test_general(dt_cor_id, gt_cor_id, 1024, 512, true_eval)\n",
    "#                     # losses = {}\n",
    "#                     # losses['2DIoU'] = torch.FloatTensor([true_eval['overall']['2DIoU']])\n",
    "#                     # losses['3DIoU'] = torch.FloatTensor([true_eval['overall']['3DIoU']])\n",
    "#                     # losses['rmse'] = torch.FloatTensor([true_eval['overall']['rmse']])\n",
    "#                     # losses['delta_1'] = torch.FloatTensor([true_eval['overall']['delta_1']])       \n",
    "\n",
    "#                     # loss_meter.update(losses['3DIoU'].mean().item(), 1)             \n",
    "#                     # print(losses['3DIoU'].mean().item())\n",
    "\n",
    "#                 # for n_corner in ['4', '6', '8', '10+', 'odd']:\n",
    "#                 #     meters[n_corner].update(sum(true_eval[n_corner]['3DIoU']), len(true_eval[n_corner]['3DIoU']))\n",
    "\n",
    "#                 # for k, v in losses.items():\n",
    "#                 #     try:                        \n",
    "#                 #         valid_loss[k] = valid_loss.get(k, 0) + v.item() * x.size(0)\n",
    "#                 #     except ValueError:                        \n",
    "#                 #         valid_loss[k] = valid_loss.get(k, 0)\n",
    "#             # for n_corner in ['4', '6', '8', '10+', 'odd']:\n",
    "#             #     print(f'{n_corner} Corners:', meters[n_corner].avg)\n",
    "\n",
    "#             # for k, v in valid_loss.items():\n",
    "#             #     k = 'valid/%s' % k\n",
    "#             #     tb_writer.add_scalar(k, v / len(dataset_valid), ith_epoch)\n",
    "\n",
    "#             # Save best validation loss model\n",
    "#             # now_valid_score = valid_loss['3DIoU'] / len(dataset_valid)\n",
    "#             now_valid_score = loss_meter.avg\n",
    "#             # scheduler.step(now_valid_score)\n",
    "#             print('Ep%3d %.4f vs. Best %.4f' % (ith_epoch, now_valid_score, args.best_valid_score))\n",
    "#             if now_valid_score <= args.best_valid_score:\n",
    "#                 args.best_valid_score = now_valid_score\n",
    "                \n",
    "#                 save_model(net,\n",
    "#                            os.path.join(args.ckpt, args.id, f'best_valid_{ith_epoch}.pth'),\n",
    "#                            args)\n",
    "#             save_model(net,\n",
    "#                        os.path.join(args.ckpt, args.id, 'last.pth'),\n",
    "#                        args)\n",
    "\n",
    "#         # Periodically save model\n",
    "#         if ith_epoch % args.save_every == 0:\n",
    "#             save_model(net,\n",
    "#                        os.path.join(args.ckpt, args.id, 'epoch_%d.pth' % ith_epoch),\n",
    "#                        args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16de64-90c8-478a-8062-54da127585a5",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f01243-be6c-4577-9133-af99cf6f8c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.cur_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46550906-dd95-4c69-9d3a-89678ff47283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7475, 1.9606],\n",
       "        [0.9826, 0.5077]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stretch_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02569a84-34d3-4a48-80e1-90ac4e34eb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8510, 1.0000],\n",
       "        [0.5920, 0.7515]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_img, src_rotation_matrix, src_scale, src_translation, target_img, target_rotation_matrix, target_scale, target_translation, stretched_src_img, stretched_target_img, stretch_k, ceiling_height = prev_params\n",
    "stretch_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0930d83c-1caa-4cfd-9bcc-97532f965b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5703, 0.7626],\n",
       "        [0.7539, 1.6063]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stretch_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e75643a-9a94-4a66-a3db-98323ed2e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters(net):\n",
    "    for p in net.parameters():\n",
    "        if torch.all(p.isnan()):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3fdd8cb-08a2-49d7-a8cb-83001a0d52a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b158f3d4-6474-48dc-9dd3-fcbcd6af622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --id stretch --train_root_dir /mnt/home_6T/sunset/zind --valid_root_dir /mnt/home_6T/sunset/zind --no_flip --no_rotate --no_gamma --batch_size_train 2\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(shlex.split(options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6785f7-4c2d-4c2f-be69-db141d073f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17dec2f-f2e7-4ac8-8f23-17fe0adb1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "options = '--id median_noshuffle --train_root_dir /mnt/home_6T/sunset/zind --valid_root_dir /mnt/home_6T/sunset/zind --no_flip --no_rotate --no_gamma --no_pano_stretch --batch_size_train 2'\n",
    "args = parser.parse_args(shlex.split(options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435aa689-6e7f-45bb-9930-35157133b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shlex.split(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6b825-1c16-4a23-a0c3-a5e74727f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374e8c1-d087-4935-bf10-9eb52d1e63c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
