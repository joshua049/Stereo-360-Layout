{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5737e64c-78d3-4faf-b427-ee81d988b653",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb84ccc-7407-48f2-8fea-b1aad6f2fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import sys\n",
    "import argparse\n",
    "import pdb\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import trange\n",
    "from copy import deepcopy\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "from model import HorizonNet, ENCODER_RESNET, ENCODER_DENSENET\n",
    "from dataset import PanoCorBonDataset, ZillowIndoorDataset, ZillowIndoorPairDataset\n",
    "from misc.utils import group_weight, adjust_learning_rate, save_model, load_trained_model\n",
    "from inference import inference\n",
    "from eval_general import test_general\n",
    "from transformations_torch import *\n",
    "from grid import warp_index, compute_local, compute_global, guess_ceiling\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, num):\n",
    "        self.val = val\n",
    "        self.sum += val * num\n",
    "        self.count += num\n",
    "        if self.count > 0:\n",
    "            self.avg = self.sum / self.count\n",
    "        else: \n",
    "            self.avg = 0\n",
    "\n",
    "def nan_to_num(x):\n",
    "    nan_map = x.isnan()\n",
    "    res = nan_map.float() #(N, 2, 1024)\n",
    "    res[:, 0, :] *= -0.478\n",
    "    res[:, 1, :] *= 0.425\n",
    "\n",
    "    x[nan_map] = 0.\n",
    "\n",
    "    return x + res    \n",
    "\n",
    "def clip_bon(bon, H):\n",
    "    # bon.shape = (N, 2, 1024)\n",
    "    margin = 10\n",
    "    ceil = torch.clamp(bon[:, 0, :], 0.5, H/2 - margin)\n",
    "    floor = torch.clamp(bon[:, 1, :], H/2 + margin, H)\n",
    "\n",
    "    return torch.stack((ceil, floor), dim=1)\n",
    "\n",
    "\n",
    "def sup_feed_forward(net, x, y_bon, y_cor):\n",
    "    x = x.to(device)\n",
    "    y_bon = y_bon.to(device)\n",
    "    y_cor = y_cor.to(device)\n",
    "    losses = {}\n",
    "\n",
    "    # y_bon_, y_cor_ = net(x)\n",
    "    y_bon_= net(x)\n",
    "    losses['bon'] = F.l1_loss(y_bon_, y_bon)\n",
    "    # losses['cor'] = F.binary_cross_entropy_with_logits(y_cor_, y_cor)    \n",
    "    # losses['total'] = losses['bon'] + losses['cor']\n",
    "    losses['total'] = losses['bon']\n",
    "\n",
    "    return losses\n",
    "\n",
    "def unsup_feed_forward(net, criterion, params, single=False, epoch=0):\n",
    "    # global _\n",
    "    # assert _ < 2500\n",
    "\n",
    "    src_img, src_rotation_matrix, src_scale, src_translation, target_img, target_rotation_matrix, target_scale, target_translation, stretched_src_img, stretched_target_img, stretch_k, ceiling_height = params\n",
    "    if single:\n",
    "        src_img = src_img[None]\n",
    "        src_rotation_matrix = src_rotation_matrix[None]\n",
    "        src_scale = src_scale[None]\n",
    "        src_translation = src_translation[None]\n",
    "        target_img = target_img[None]\n",
    "        target_rotation_matrix = target_rotation_matrix[None]\n",
    "        target_scale = target_scale[None]\n",
    "        target_translation = target_translation[None]\n",
    "        stretched_src_img = stretched_src_img[None]\n",
    "        stretched_target_img = stretched_target_img[None]\n",
    "        stretch_k = stretch_k[None]\n",
    "        ceiling_height = ceiling_height[None]\n",
    "    \n",
    "    src_img = src_img.to(device)\n",
    "    src_rotation_matrix = src_rotation_matrix.to(device)\n",
    "    src_scale = src_scale.to(device)\n",
    "    src_translation = src_translation.to(device)\n",
    "    target_img = target_img.to(device)\n",
    "    target_rotation_matrix = target_rotation_matrix.to(device)\n",
    "    target_scale = target_scale.to(device)\n",
    "    target_translation = target_translation.to(device)\n",
    "    stretched_target_img = stretched_target_img.to(device)\n",
    "    stretch_k = stretch_k.to(device)\n",
    "    ceiling_height = ceiling_height.to(device)\n",
    "\n",
    "    N, C, H, W = src_img.shape\n",
    "\n",
    "    y_bon_ori = net(target_img)\n",
    "    assert not torch.all(torch.isnan(y_bon_ori))\n",
    "    y_bon_ori = nan_to_num(y_bon_ori)\n",
    "\n",
    "    y_bon = (y_bon_ori / 2 + 0.5) * H - 0.5\n",
    "    y_bon = clip_bon(y_bon, H)\n",
    "\n",
    "    losses = {} \n",
    "\n",
    "    src_transformer = Transformation2D(rotation_matrix=src_rotation_matrix, scale=src_scale[:, :, None], translation=src_translation)\n",
    "    target_transformer = Transformation2D(rotation_matrix=target_rotation_matrix, scale=target_scale[:, :, None], translation=target_translation)\n",
    "    \n",
    "    ceiling_z = ceiling_height - 1.\n",
    "\n",
    "    grid = warp_index(src_transformer, target_transformer, y_bon, H, W, ceiling_z) \n",
    "    assert not torch.any(torch.isnan(grid))    \n",
    "\n",
    "    warp_img = F.grid_sample(src_img, grid)\n",
    "\n",
    "    margin = 45\n",
    "    src_valid_map = torch.cat([torch.ones(N, C, H - margin, W), torch.zeros(N, C, margin, W)], axis=2).to(device)\n",
    "    target_valid_map = F.grid_sample(src_valid_map, grid)\n",
    "    valid_map = torch.clip(src_valid_map * target_valid_map, min=0., max=1.).detach()\n",
    "\n",
    "    losses['ph'] = (criterion(warp_img, target_img) * valid_map).mean()\n",
    "    assert not torch.any(torch.isnan(warp_img))\n",
    "\n",
    "\n",
    "    pseudo_y_bon_ori = nan_to_num(net(warp_img.detach()))\n",
    "    pseudo_y_bon = (pseudo_y_bon_ori / 2 + 0.5) * H - 0.5\n",
    "    pseudo_y_bon = clip_bon(pseudo_y_bon, H)\n",
    "\n",
    "\n",
    "    target_local_2d = compute_local(y_bon, H, W, ceiling_z)\n",
    "    target_global_2d = compute_global(y_bon, target_transformer, H, W, ceiling_z)   \n",
    "\n",
    "    target_local_stretch = target_local_2d * stretch_k[:, None, :]\n",
    "    \n",
    "    stretched_y_bon_ = net(stretched_target_img)\n",
    "    stretched_y_bon_ = (stretched_y_bon_ / 2 + 0.5) * H - 0.5  \n",
    "    stretched_y_bon_ = clip_bon(stretched_y_bon_, H)\n",
    "    target_local_stretch_ = compute_local(stretched_y_bon_, H, W, ceiling_z)\n",
    "\n",
    "    target_global_2d -= target_translation\n",
    "\n",
    "    ceil = target_global_2d[:, :1024, :]\n",
    "    floor = target_global_2d[:, 1024:, :]\n",
    "    losses['ceil_floor'] = criterion(ceil, floor).mean()\n",
    "\n",
    "    \n",
    "    kernel_size = 15\n",
    "    unfold = nn.Unfold(kernel_size=(1, kernel_size))\n",
    "    windows = unfold(target_global_2d.reshape(N, 2, W, 2).permute(0, 1, 3, 2)).reshape(N, 2, 2, -1, kernel_size) #(N, C, XY, L, K)\n",
    "    windows_mean = windows.median(dim=-1, keepdim=True).values\n",
    "    windows_slope = windows / windows.norm(dim=2, keepdim=True)\n",
    "    \n",
    "    \n",
    "    losses['bon'] = criterion(pseudo_y_bon_ori, y_bon_ori.detach()).mean()\n",
    "    losses['stretch'] = chamfer_distance(target_local_stretch.detach(), target_local_stretch_)[0]\n",
    "    losses['parallel'] = torch.abs((windows - windows_mean) * torch.flip(windows_slope, [2])).mean(dim=4).min(dim=2).values.mean()\n",
    "    losses['total'] = losses['ph'] + losses['bon'] * 0.1 + losses['parallel'] * 0.1 + losses['ceil_floor'] * 0.15 + losses['stretch'] * 0.1\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2be76-a55e-4bf4-84da-3adcd5bc09ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup: 1139\n",
      "UnSup: 5521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                                                                                         | 0/20 [00:00<?, ?ep/s]\n",
      "Unsup Train ep1:   0%|                                                                                                                                             | 0/2760 [00:00<?, ?it/s]\u001b[A/home/joshua049/anaconda3/envs/pytorch3d/lib/python3.8/site-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
      "\n",
      "Unsup Train ep1:   0%|                                                                                                                                   | 1/2760 [00:02<1:32:35,  2.01s/it]\u001b[A\n",
      "Unsup Train ep1:   0%|                                                                                                                                   | 2/2760 [00:02<1:02:35,  1.36s/it]\u001b[A\n",
      "Unsup Train ep1:   0%|▏                                                                                                                                    | 3/2760 [00:03<53:10,  1.16s/it]\u001b[A\n",
      "Unsup Train ep1:   0%|▏                                                                                                                                    | 4/2760 [00:04<48:35,  1.06s/it]\u001b[A\n",
      "Unsup Train ep1:   0%|▏                                                                                                                                    | 5/2760 [00:05<46:02,  1.00s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--id', required=True,\n",
    "                        help='experiment id to name checkpoints and logs')\n",
    "    parser.add_argument('--ckpt', default='./ckpt',\n",
    "                        help='folder to output checkpoints')\n",
    "    parser.add_argument('--logs', default='./logs',\n",
    "                        help='folder to logging')\n",
    "    parser.add_argument('--pth', default=None,\n",
    "                        help='path to load saved checkpoint.'\n",
    "                             '(finetuning)')\n",
    "    parser.add_argument('--log_to_file', action='store_true',\n",
    "                        help='redirect stdout to logs/{id}.log')\n",
    "    # Model related\n",
    "    parser.add_argument('--backbone', default='resnet50',\n",
    "                        choices=ENCODER_RESNET + ENCODER_DENSENET,\n",
    "                        help='backbone of the network')\n",
    "    parser.add_argument('--no_rnn', action='store_true',\n",
    "                        help='whether to remove rnn or not')\n",
    "    # Dataset related arguments\n",
    "    parser.add_argument('--train_root_dir', default='data/layoutnet_dataset/train',\n",
    "                        help='root directory to training dataset. '\n",
    "                             'should contains img, label_cor subdirectories')\n",
    "    parser.add_argument('--valid_root_dir', default='data/layoutnet_dataset/valid',\n",
    "                        help='root directory to validation dataset. '\n",
    "                             'should contains img, label_cor subdirectories')\n",
    "    parser.add_argument('--no_flip', action='store_true',\n",
    "                        help='disable left-right flip augmentation')\n",
    "    parser.add_argument('--no_rotate', action='store_true',\n",
    "                        help='disable horizontal rotate augmentation')\n",
    "    parser.add_argument('--no_gamma', action='store_true',\n",
    "                        help='disable gamma augmentation')\n",
    "    parser.add_argument('--no_pano_stretch', action='store_true',\n",
    "                        help='disable pano stretch')\n",
    "    parser.add_argument('--num_workers', default=6, type=int,\n",
    "                        help='numbers of workers for dataloaders')\n",
    "    parser.add_argument('--sup_ratio', default=0.2, type=float,\n",
    "                        help='ratio of supervised data')\n",
    "    # optimization related arguments\n",
    "    parser.add_argument('--freeze_earlier_blocks', default=-1, type=int)\n",
    "    parser.add_argument('--batch_size_train', default=4, type=int,\n",
    "                        help='training mini-batch size')\n",
    "    parser.add_argument('--batch_size_valid', default=2, type=int,\n",
    "                        help='validation mini-batch size')\n",
    "    parser.add_argument('--epochs', default=300, type=int,\n",
    "                        help='epochs to train')\n",
    "    parser.add_argument('--optim', default='Adam',\n",
    "                        help='optimizer to use. only support SGD and Adam')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_pow', default=0.9, type=float,\n",
    "                        help='power in poly to drop LR')\n",
    "    parser.add_argument('--warmup_lr', default=1e-6, type=float,\n",
    "                        help='starting learning rate for warm up')\n",
    "    parser.add_argument('--warmup_epochs', default=0, type=int,\n",
    "                        help='numbers of warmup epochs')\n",
    "    parser.add_argument('--beta1', default=0.9, type=float,\n",
    "                        help='momentum for sgd, beta1 for adam')\n",
    "    parser.add_argument('--weight_decay', default=0, type=float,\n",
    "                        help='factor for L2 regularization')\n",
    "    parser.add_argument('--bn_momentum', type=float)\n",
    "    # Misc arguments\n",
    "    parser.add_argument('--no_cuda', action='store_true',\n",
    "                        help='disable cuda')\n",
    "    parser.add_argument('--seed', default=594277, type=int,\n",
    "                        help='manual seed')\n",
    "    parser.add_argument('--disp_iter', type=int, default=1,\n",
    "                        help='iterations frequency to display')\n",
    "    parser.add_argument('--save_every', type=int, default=25,\n",
    "                        help='epochs frequency to save state_dict')\n",
    "    \n",
    "    dataset = '--train_root_dir /mnt/home_6T/sunset/zind --valid_root_dir /mnt/home_6T/sunset/zind'\n",
    "    exid = 'validmap'\n",
    "    options = f'--id {exid} {dataset} --batch_size_train 2 --sup_ratio 0.05 --epochs 20'\n",
    "    \n",
    "    \n",
    "    args = parser.parse_args(shlex.split(options))\n",
    "    device = torch.device('cpu' if args.no_cuda else 'cuda')\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    os.makedirs(os.path.join(args.ckpt, args.id), exist_ok=True)\n",
    "\n",
    "    # torch.manual_seed(2021)\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # if args.log_to_file:\n",
    "    #     sys.stdout = open(f'{args.logs}/{args.id}.log')\n",
    "\n",
    "\n",
    "    # Create dataloader\n",
    "    # dataset_train = PanoCorBonDataset(\n",
    "    with open(os.path.join(args.train_root_dir, 'zind_partition.json')) as f: split_data = json.load(f)\n",
    "    num_train_scenes = len(split_data['train'])\n",
    "    split_index = int(num_train_scenes * args.sup_ratio)\n",
    "\n",
    "    sup_dataset_train = ZillowIndoorDataset(\n",
    "        root_dir=args.train_root_dir,\n",
    "        subject='train',\n",
    "        flip=not args.no_flip, rotate=not args.no_rotate, gamma=not args.no_gamma,\n",
    "        stretch=not args.no_pano_stretch,\n",
    "        start=None, end=split_index)\n",
    "    sup_loader_train = DataLoader(sup_dataset_train, args.batch_size_train * 2,\n",
    "                              shuffle=True, drop_last=True,\n",
    "                              num_workers=args.num_workers,\n",
    "                              pin_memory=not args.no_cuda,\n",
    "                              worker_init_fn=lambda x: np.random.seed())\n",
    "\n",
    "    unsup_dataset_train = ZillowIndoorPairDataset(\n",
    "        root_dir=args.train_root_dir,\n",
    "        subject='train',\n",
    "        flip=False, rotate=False, gamma=False,\n",
    "        stretch=True,\n",
    "        max_stretch=1.5,\n",
    "        start=split_index)\n",
    "    unsup_loader_train = DataLoader(unsup_dataset_train, args.batch_size_train,\n",
    "                              shuffle=True, drop_last=True,\n",
    "                              num_workers=args.num_workers,\n",
    "                              pin_memory=not args.no_cuda,\n",
    "                              worker_init_fn=lambda x: np.random.seed())\n",
    "\n",
    "    print('Sup:', len(sup_dataset_train))\n",
    "    print('UnSup:', len(unsup_dataset_train))\n",
    "\n",
    "    # assert False\n",
    "\n",
    "\n",
    "    if args.valid_root_dir:\n",
    "        # dataset_valid = PanoCorBonDataset(\n",
    "        dataset_valid = ZillowIndoorDataset(\n",
    "            root_dir=args.valid_root_dir, subject='val', return_cor=True,\n",
    "            flip=False, rotate=False, gamma=False,\n",
    "            stretch=False)\n",
    "\n",
    "    # Create model\n",
    "    if args.pth is not None:\n",
    "        print('Finetune model is given.')\n",
    "        print('Ignore --backbone and --no_rnn')\n",
    "        net = load_trained_model(HorizonNet, args.pth).to(device)\n",
    "    else:\n",
    "        net = HorizonNet(args.backbone, not args.no_rnn).to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    # doornet = DoorNet()\n",
    "    # doornet.load_state_dict(torch.load('./ckpt/door_sub/best.pth')['state_dict'])\n",
    "    doornet = None\n",
    "    \n",
    "    \n",
    "    assert -1 <= args.freeze_earlier_blocks and args.freeze_earlier_blocks <= 4\n",
    "    if args.freeze_earlier_blocks != -1:\n",
    "        b0, b1, b2, b3, b4 = net.feature_extractor.list_blocks()\n",
    "        blocks = [b0, b1, b2, b3, b4]\n",
    "        for i in range(args.freeze_earlier_blocks + 1):\n",
    "            print('Freeze block%d' % i)\n",
    "            for m in blocks[i]:\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    if args.bn_momentum:\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                m.momentum = args.bn_momentum\n",
    "\n",
    "    # Create optimizer\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, net.parameters()),\n",
    "            lr=args.lr, momentum=args.beta1, weight_decay=args.weight_decay)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, net.parameters()),\n",
    "            lr=args.lr, betas=(args.beta1, 0.999), weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, min_lr=1e-6)\n",
    "    # Create tensorboard for monitoring training\n",
    "    tb_path = os.path.join(args.logs, args.id)\n",
    "    os.makedirs(tb_path, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(log_dir=tb_path)\n",
    "\n",
    "    # Init variable\n",
    "    args.warmup_iters = args.warmup_epochs * (len(sup_loader_train) + len(unsup_loader_train))\n",
    "    args.max_iters = args.epochs * (len(sup_loader_train) + len(unsup_loader_train))\n",
    "    args.running_lr = args.warmup_lr if args.warmup_epochs > 0 else args.lr\n",
    "    args.cur_iter = 0\n",
    "    args.best_valid_score = -1\n",
    "\n",
    "    # Start training\n",
    "    for ith_epoch in trange(1, args.epochs + 1, desc='Epoch', unit='ep'):\n",
    "\n",
    "        # Train phase\n",
    "        net.train()\n",
    "        if args.freeze_earlier_blocks != -1:\n",
    "            b0, b1, b2, b3, b4 = net.feature_extractor.list_blocks()\n",
    "            blocks = [b0, b1, b2, b3, b4]\n",
    "            for i in range(args.freeze_earlier_blocks + 1):\n",
    "                for m in blocks[i]:\n",
    "                    m.eval()\n",
    "        \n",
    "        if ith_epoch > args.epochs // 2:\n",
    "            iterator_train = iter(sup_loader_train)\n",
    "            for _ in trange(len(sup_loader_train),\n",
    "                            desc='Sup Train ep%s' % ith_epoch, position=1):\n",
    "                # Set learning rate\n",
    "                adjust_learning_rate(optimizer, args)\n",
    "\n",
    "                args.cur_iter += 1\n",
    "                # print('*'*30, args.cur_iter, '*'*30)\n",
    "                x, y_bon, y_cor = next(iterator_train)\n",
    "\n",
    "                losses = sup_feed_forward(net, x, y_bon, y_cor)\n",
    "                for k, v in losses.items():\n",
    "                    k = 'train/%s' % k\n",
    "                    tb_writer.add_scalar(k, v.item(), args.cur_iter)\n",
    "                tb_writer.add_scalar('train/lr', args.running_lr, args.cur_iter)\n",
    "                loss = losses['total']\n",
    "\n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), 3.0, norm_type='inf')\n",
    "                optimizer.step()\n",
    "\n",
    "                # break\n",
    "        else:\n",
    "            loss_history = [None] * 10\n",
    "            model_history = [None] * 10\n",
    "            \n",
    "            iterator_train = iter(unsup_loader_train)\n",
    "            for _ in trange(len(unsup_loader_train),\n",
    "                            desc='Unsup Train ep%s' % ith_epoch, position=1):\n",
    "                # Set learning ratetmux\n",
    "                adjust_learning_rate(optimizer, args)\n",
    "\n",
    "                args.cur_iter += 1\n",
    "                # print('*'*30, args.cur_iter, '*'*30)\n",
    "                params = next(iterator_train)\n",
    "\n",
    "                try:\n",
    "                    losses = unsup_feed_forward(net, criterion, params, epoch=ith_epoch)\n",
    "                    \n",
    "                except AssertionError as e:\n",
    "                    for i, loss_it in enumerate(loss_history):\n",
    "                        print(f'Ep {i+1}:', loss_it)\n",
    "                    \n",
    "                    assert False\n",
    "                    \n",
    "                for k, v in losses.items():\n",
    "                    k = 'train/%s' % k\n",
    "                    tb_writer.add_scalar(k, v.item(), args.cur_iter)\n",
    "                tb_writer.add_scalar('train/lr', args.running_lr, args.cur_iter)\n",
    "\n",
    "                loss_history.pop(0)\n",
    "                loss_history.append(losses)\n",
    "                \n",
    "                model_history.pop(0)\n",
    "                model_history.append(deepcopy(net).to('cpu'))\n",
    "\n",
    "                for term in ['total']:\n",
    "                    loss = losses[term]\n",
    "\n",
    "                    # backprop\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    nn.utils.clip_grad_norm_(net.parameters(), 1.0, norm_type='inf')\n",
    "                    optimizer.step()\n",
    "        \n",
    "        # Valid phase\n",
    "        net.eval()\n",
    "        if args.valid_root_dir:\n",
    "            valid_loss = {}\n",
    "            meters = {n_corner: AverageMeter() for n_corner in ['4', '6', '8', '10+', 'odd']}\n",
    "            excepts = 0\n",
    "            for jth in trange(len(dataset_valid),\n",
    "                            desc='Valid ep%d' % ith_epoch, position=2):\n",
    "                x, y_bon, y_cor, gt_cor_id = dataset_valid[jth]\n",
    "                x, y_bon, y_cor = x[None], y_bon[None], y_cor[None]\n",
    "                with torch.no_grad():\n",
    "                    losses = sup_feed_forward(net, x, y_bon, y_cor)\n",
    "\n",
    "                    # True eval result instead of training objective\n",
    "                    true_eval = dict([\n",
    "                        (n_corner, {'2DIoU': [], '3DIoU': [], 'rmse': [], 'delta_1': []})\n",
    "                        for n_corner in ['4', '6', '8', '10+', 'odd', 'overall']\n",
    "                    ])\n",
    "\n",
    "                    \n",
    "                    # dt_cor_id = inference(net, doornet, x, device, force_cuboid=False)[0]\n",
    "                    # dt_cor_id[:, 0] *= 1024\n",
    "                    # dt_cor_id[:, 1] *= 512\n",
    "                    try:\n",
    "                        dt_cor_id = inference(net, doornet, x, device, force_cuboid=False, force_raw=True)[0]\n",
    "                        dt_cor_id[:, 0] *= 1024\n",
    "                        dt_cor_id[:, 1] *= 512\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        excepts += 1\n",
    "                        dt_cor_id = np.array([\n",
    "                            [k//2 * 1024, 256 - ((k%2)*2 - 1) * 120]\n",
    "                            for k in range(8)\n",
    "                        ])\n",
    "                    test_general(dt_cor_id, gt_cor_id, 1024, 512, true_eval)\n",
    "                    losses['2DIoU'] = torch.FloatTensor([true_eval['overall']['2DIoU']])\n",
    "                    losses['3DIoU'] = torch.FloatTensor([true_eval['overall']['3DIoU']])\n",
    "                    losses['rmse'] = torch.FloatTensor([true_eval['overall']['rmse']])\n",
    "                    losses['delta_1'] = torch.FloatTensor([true_eval['overall']['delta_1']]) \n",
    "\n",
    "                    # print(losses['3DIoU'])                   \n",
    "\n",
    "                for n_corner in ['4', '6', '8', '10+', 'odd']:\n",
    "                    meters[n_corner].update(sum(true_eval[n_corner]['3DIoU']), len(true_eval[n_corner]['3DIoU']))\n",
    "\n",
    "                for k, v in losses.items():\n",
    "                    try:                        \n",
    "                        valid_loss[k] = valid_loss.get(k, 0) + v.item() * x.size(0)\n",
    "                    except ValueError:                        \n",
    "                        valid_loss[k] = valid_loss.get(k, 0)\n",
    "            print('Num of Exceptions:', excepts)\n",
    "            for n_corner in ['4', '6', '8', '10+', 'odd']:\n",
    "                print(f'{n_corner} Corners:', meters[n_corner].avg)\n",
    "\n",
    "            for k, v in valid_loss.items():\n",
    "                k = 'valid/%s' % k\n",
    "                tb_writer.add_scalar(k, v / len(dataset_valid), ith_epoch)\n",
    "\n",
    "            # Save best validation loss model\n",
    "            now_valid_score = valid_loss['3DIoU'] / len(dataset_valid)\n",
    "            # now_valid_score = loss_meter.avg\n",
    "            # scheduler.step(now_valid_score)\n",
    "            print('Ep%3d %.4f vs. Best %.4f' % (ith_epoch, now_valid_score, args.best_valid_score))\n",
    "            if now_valid_score >= args.best_valid_score:\n",
    "                args.best_valid_score = now_valid_score\n",
    "                \n",
    "                save_model(net,\n",
    "                           os.path.join(args.ckpt, args.id, f'best_valid_{ith_epoch}.pth'),\n",
    "                           args)\n",
    "            save_model(net,\n",
    "                       os.path.join(args.ckpt, args.id, 'last.pth'),\n",
    "                       args)\n",
    "\n",
    "        # Periodically save model\n",
    "        if ith_epoch % args.save_every == 0:\n",
    "            save_model(net,\n",
    "                       os.path.join(args.ckpt, args.id, 'epoch_%d.pth' % ith_epoch),\n",
    "                       args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684eac39-53d0-42b5-b905-cd4d36aea091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 50, 'Done', '-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da737f33-07b1-4cff-a9ad-5685f056b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ph': tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0119, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0180, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0193, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0236, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0091, device='cuda:0', grad_fn=<AddBackward0>)}, {'ph': tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>), 'ceil_floor': tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>), 'bon': tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>), 'stretch': tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>), 'parallel': tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>), 'total': tensor(0.0081, device='cuda:0', grad_fn=<AddBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "print(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6700626d-0a3b-46b7-9d94-946334d47581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for net_i in model_history:\n",
    "    for p in net_i.parameters():\n",
    "        if torch.all(torch.isnan(p)):\n",
    "            print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16de64-90c8-478a-8062-54da127585a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f560a-14a2-4d48-96e0-42ca5cede166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17dec2f-f2e7-4ac8-8f23-17fe0adb1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "options = '--id median_noshuffle --train_root_dir /mnt/home_6T/sunset/zind --valid_root_dir /mnt/home_6T/sunset/zind --no_flip --no_rotate --no_gamma --no_pano_stretch --batch_size_train 2'\n",
    "args = parser.parse_args(shlex.split(options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435aa689-6e7f-45bb-9930-35157133b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shlex.split(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6b825-1c16-4a23-a0c3-a5e74727f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374e8c1-d087-4935-bf10-9eb52d1e63c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
